\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{multicol}
% maxamize space on paper
\usepackage[nomarginpar, margin=.1in]{geometry}
\usepackage{sectsty}
\usepackage{amsmath}
\sectionfont{\fontsize{9}{9}\selectfont}
\subsectionfont{\fontsize{9}{9}\selectfont}
\subsubsectionfont{\fontsize{9}{9}\selectfont}
\usepackage[compact]{titlesec}
\usepackage{enumitem}
\setlist[1]{itemsep=-5pt}
\setlist[2]{itemsep=-5pt}
\begin{document}
\underline{This document is currently incomplete - Information may be incorrect - Please contribute on GitHub!}
\section{Divide and Conquer}
Break up a problem into sub-problems, solve each sub-problem independently and combine the solutions to sub-problems to form a
solution to the original problem
\subsection{Solving Recurrences}
\subsubsection{via Unrolling}
To solve the recurrence via unrolling, you plot the values of the size of the problem and
the time taken to solve the problem in the table to find a generalized equation (see below). For example,
merge sort's recurrence is: $T(n) = 2T(\frac{n}{2}) + n$ where $T(1) = 0$

\begin{multicols}{2}
\begin{center}
    \begin{tabular}{c c c}
        \underline{Level} & \underline{Size} & \underline{Time} \\
        0 & $n$ & $n = n$ \\
        1 & $n/2$ & $2(n/2) = n$ \\
        2 & $n/4$ & $4(n/4) = n$ \\
        k & $n/2^k$ & $2^k(n/2^k) = n$ \\
    \end{tabular}
\end{center}
\columnbreak
You can then multiply your generalized time function (in this case just $n$) by the number of levels
to get the runtime. You can get the levels by solving the following for $k$: $1 = n/2^k$ where $1$ represents the value of $n$
at the base case and where the righthand side is the generalized function found in the size column.
In this case, $k \approx log(n)$, therefore the runtime is $O(n*log(n))$
\end{multicols}
\subsubsection{via Master Theorem}
In order to use the master theorem to solve a problem, the recurrence must be of the form:
$T(n) = aT(\frac{n}{b})+f(n)$
There are three cases for the master theorem, in each case it must be \textbf{polynominally} larger/smaller for the case to apply.

\begin{enumerate}
    \item (Leaf-Heavy) if $f(n) < n^{log_b(a)}$, then  $T(n) = O(n^{log_b(a)})$
    \item if $f(n) = n^{log_b(a)}$, then $T(n) = O(n^{log_b(a)}log(n))$
    \item (Root-Heavy) if $f(n) > n^{log_b(a)}$, then $T(n) = O(f(n))$
\end{enumerate}

\subsection{Merge Sort}
Merge sort is a sorting algorithm that works by dividing an array into smaller subarrays, sorting each subarray, and then merging the sorted subarrays back together to form the final sorted array. It has a time complexity of $O(n*log(n))$ (see 1.1.1)

\subsection{Closest Pair}
Given an array $A$ of $n$ points, find the 2 points that are the closest to each other. 
The solution to this works by dividing $A$ into 2 halves (with $w$ being the middlemost element) and recursively finding the smallest
distance between 2 nodes in a subarray. Let $d$ be the minimum of the shortest distance between
the 2 subarrays. We then find all nodes whose x-coordinate is closer to $w_x$ than $d$, and
put them into an array $S$. We now find the lowest distance in $S$, if it is lower than $d$, 
then we set $d$ equal to it. We then finally return $d$. TC: $O(n*log(n)^2)$

\section{Dynamic Programming}
Break up a problem into a series of overlapping sub-problems and build up a solution to larger sub-problems.
The basic idea behind many DP algorithms is to design a recurrence where we take the max/min of adding the current
object to the result. When proving a DP algo, typically you prove 2 things: that the recurrence you choose is right, and 
that your algorithm correctly implements the recurrence

\subsection{Weighted Interval Scheduling}
Same as interval scheduling problem except that each interval has a weight attached to it. 
The recurrence $OPT(i) = max[OPT(v_i) + p_i, OPT(i - 1)]$ where $i$ is the $i^{th}$ router, $v(i)$
is a function that returns the interval immeadeatly preceeding the $i^{th}$ interval without overlapping with it, 
and $p_i$ is the weight of the $i^{th}$ interval. The runtime for this solution is $O(n)$ when $v(i)$ is 
represented with a memoized array (see 2.2).

\subsection{Memoization}
Speeds up algorithms by eliminating the repetitive computation of results \& avoiding repeated function calls that process the same input.
As an example, in (2.1), if we used a function $v(i)$, then our runtime would be $O(n^2)$, however if we created an array $V$ and ran $v(i)$
on all $i \leq 0 \leq num\_intervals$  and stored it in $V[i]$, then our runtime would be $O(n)$

\subsection{Subset Sums and Knapsacks}
The goal of this problem is to find the maximum set of weights whose sum is less than $W$.
The recurrence used to solve this is $OPT(i, W) = max[w_i + OPT(i - 1, W - w_i), OPT(i-1, W)]$ (if $w_i < W$ then: $OPT(i, W) = OPT(i-1, W)$).
Where $W$ is the max weight, $w_i$ is the weight of object $i$. Typically, the output from the OPT 
function is memoized into a 2d array, resulting in a pesudo-polynomial time complexity of $O(n * W)$

\subsection{Sequence Alignment}
Given two strings X and Y, an alignment M of X and Y is obtained by inserting spaces into or
before or after the ends of X and Y so that the resulting two strings $X'$ and $Y'$  have the samenumber of characters.
The cost of an alignment will be $(g * c_g) + (m + c_m)$ where $g$ is the gaps added, $m$ is the mismatches, and $c_g$, $c_m$
are the costs for adding a gap and having a mismatch, respectively.
The recurrence representing this is: $OPT(i, j) = min[a_{x_iy_j} + OPT(i - 1, j - 1),\delta + OPT(i - 1, j), \delta + OPT(i, j - 1)]$

\noindent The output of $OPT$ is typically memoized into a 2d array, and results in a runtime of $O(m * n)$ where $m$ and $n$ represent
the length of each of the strings.

\subsection{Bellman-Ford Algorithm}
\begin{multicols}{2}
Accomplishes the same thing as Djikstra's algorithms while also allowing for negative
weight edges. The recurrence for solving this is on the right.
Where $w$ represents an intermediate vertex, $c_{vw}$ represents the cost from going from $v$ to $w$,
$v$ represents the starting vertex, and $i$ represents the length of the path. 
\columnbreak

$OPT(n) = \begin{cases}
    OPT(i-1, v)  & P \text{ uses at most } i-1 \text{edges} \\
    OPT(i-1, w) + c_{vw} & P \text{ uses at most } i \text{ edges}
  \end{cases}$
\end{multicols}

\section{Network Flow}
Graphs are sometimes used to model transportation networks. In
such networks, edges carry some sort of traffic and nodes act like switches passing the
traffic between edges. For example, in a computer network, the edges are links that carry
the data and the nodes are switches. Network flow are used to model such transportation
networks.

\subsection{Maximum Flow Problem}
The max flow problem represents a problem in which we want to find the maximum flow between
2 edges (a source, typically denoted with $s$, and a sink, typically denoted with $t$). 

\subsection{Ford-Fulkerson Algorithm}
The Ford-Fulkerson Algorithm works by finding a simple path $p$ beetween $s$ and $t$, then finding
the edge with the least capacity on that path and augmenting each node with that value.
Note that you can go against the direction of an edge in the graph so long as you subtract when augmenting
it's capacity.
This is then repeated until no paths between $s$ and $t$ with a nonzero capacity exist. 

\subsection{Maximum Flow and Minimum Cut in a Network}
The min cut refers to the minimum set of edges that would have to be removed such that there is no longer
a path from $s$ to $t$. The capacity of the cut edges is equal to the maximum flow of the network.

\end{document}

