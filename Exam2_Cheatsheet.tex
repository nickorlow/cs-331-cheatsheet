\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{multicol}
% maxamize space on paper
\usepackage[nomarginpar, margin=.1in]{geometry}
\usepackage{sectsty}
\usepackage{amsmath}
\sectionfont{\fontsize{9}{9}\selectfont}
\subsectionfont{\fontsize{9}{9}\selectfont}
\subsubsectionfont{\fontsize{9}{9}\selectfont}
\usepackage[compact]{titlesec}
\usepackage{enumitem}
\setlist[1]{itemsep=-5pt}
\setlist[2]{itemsep=-5pt}
\begin{document}
%\underline{This document is currently incomplete - Information may be incorrect - Please contribute on GitHub!}
\section{Divide and Conquer}
Break up a problem into sub-problems, solve each sub-problem independently and combine the solutions to sub-problems to form a
solution to the original problem
\subsection{Solving Recurrences}
\subsubsection{via Unrolling}
To solve the recurrence via unrolling, you plot the values of the size of the problem and
the time taken to solve the problem in the table to find a generalized equation (see below). For example,
merge sort's recurrence is: $T(n) = 2T(\frac{n}{2}) + n$ where $T(1) = 0$. In general, a is
the number of sub problems and it is multiplied by the size of each sub problem. 

\begin{multicols}{2}
\begin{center}
    \begin{tabular}{c c c}
        \underline{Level} & \underline{Size} & \underline{Time} \\
        0 & $n$ & $n = n$ \\
        1 & $n/2$ & $2(n/2) = n$ \\
        2 & $n/4$ & $4(n/4) = n$ \\
        k & $n/2^k$ & $2^k(n/2^k) = n$ \\
    \end{tabular}
\end{center}
\columnbreak
You can then multiply your generalized time function (in this case just $n$) by the number of levels
to get the runtime. You can get the levels by solving the following for $k$: $1 = n/2^k$ where $1$ represents the value of $n$
at the base case and where the righthand side is the generalized function found in the size column.
In this case, $k \approx log(n)$, therefore the runtime is $O(n*log(n))$
\end{multicols}
\subsubsection{via Master Theorem}
In order to use the master theorem to solve a problem, the recurrence must be of the form:
$T(n) = aT(\frac{n}{b})+f(n)$ where a and b are constants.
There are three cases for the master theorem, in each case it must be \textbf{polynominally} larger/smaller for the case to apply.

\begin{enumerate}
    \item (Leaf-Heavy) if $O(f(n)) < n^{log_b(a)}$, then  $T(n) = O(n^{log_b(a)})$
    \begin{itemize}
        \item polynomially smaller: $f(n)$ must be smaller than $n^{\log_b(a)}$ by a factor of $n^{\epsilon}$
    \end{itemize}
    \item if $O(f(n)) = n^{log_b(a)}$, then $T(n) = O(n^{log_b(a)}log(n))$
    \item (Root-Heavy) if $O(f(n)) > n^{log_b(a)}$, then $T(n) = O(f(n))$ (Note: must also satisfy the regularity condition $af(n/b) \leq cf(n)$, $c < 1$)
    \begin{itemize}
        \item polynomially larger: $f(n)$ must be smaller than $n^{\log_b(a)}$ by a factor of $n^{\epsilon}$
    \end{itemize}
\end{enumerate}

Note: In discussion section 5, it was shown that when $f(n) = n^2+n+1$, and the righthandside of the comparision was
$n^2$ that case 2 applied since $f(n) \in O(n^2)$


\subsection{Merge Sort}
Merge sort is a sorting algorithm that works by dividing an array into smaller subarrays, sorting each subarray, and then merging the sorted subarrays back together to form the final sorted array. It has a time complexity of $O(n*log(n))$ (see 1.1.1)

\subsection{Closest Pair}
Given an array $A$ of $n$ points, find the 2 points that are the closest to each other. 
The solution to this works by dividing $A$ into 2 halves (with $w$ being the middlemost element) and recursively finding the smallest
distance between 2 nodes in a subarray. Let $d$ be the minimum of the shortest distance between
the 2 subarrays. We then find all nodes whose x-coordinate is closer to $w_x$ than $d$, and
put them into an array $S$. We now find the lowest distance in $S$, if it is lower than $d$, 
then we set $d$ equal to it. We then finally return $d$. TC: $O(n*log(n)^2)$

\section{Dynamic Programming}
Break up a problem into a series of overlapping sub-problems and build up a solution to larger sub-problems.
The basic idea behind many DP algorithms is to design a recurrence where we take the max/min of either including or excluding the current
object to the result. When proving a DP algo, typically you prove 2 things: that the recurrence you choose is right, and 
that your algorithm correctly implements the recurrence

\subsection{Weighted Interval Scheduling}
Same as interval scheduling problem except that each interval has a weight attached to it. 
The recurrence $OPT(i) = max[OPT(v_i) + p_i, OPT(i - 1)]$ where $i$ is the $i^{th}$ interval, $v(i)$
is a function that returns the interval immediately preceding the $i^{th}$ interval without overlapping with it, 
and $p_i$ is the weight of the $i^{th}$ interval. The runtime for this solution is $O(n)$ when $v(i)$ is 
represented with a memoized array (see 2.2).

\subsection{Memoization}
Speeds up algorithms by eliminating the repetitive computation of results \& avoiding repeated function calls that process the same input.
As an example, in (2.1), if we used a function $v(i)$, then our runtime would be $O(n^2)$, however if we created an array $V$ and ran $v(i)$
on all $i \leq 0 \leq num\_intervals$  and stored it in $V[i]$, then our runtime would be $O(n)$

\subsection{Subset Sums and Knapsacks}
The goal of this problem is to find the maximum set of weights whose sum is less than $W$.
The recurrence used to solve this is $OPT(i, W) = max[w_i + OPT(i - 1, W - w_i), OPT(i-1, W)]$ (if $w_i < W$ then: $OPT(i, W) = OPT(i-1, W)$).
Where $W$ is the max weight, $w_i$ is the weight of object $i$. Typically, the output from the OPT 
function is memoized into a 2d array, resulting in a pseudo-polynomial (see section 4) time complexity of $O(n * W)$
\vspace{10em}
\subsection{Sequence Alignment}

\begin{multicols}{2}
Given two strings X and Y, an alignment M of X and Y is obtained by inserting spaces into or
before or after the ends of X and Y so that the resulting two strings $X'$ and $Y'$  have the same number of characters.
The cost of an alignment will be $(g * c_g) + (m + c_m)$ where $g$ is the gaps added, $m$ is the mismatches, and $c_g$, $c_m$
are the costs for adding a gap and having a mismatch, respectively.
The recurrence representing this is: $OPT(i, j) = min[a_{x_iy_j} + OPT(i - 1, j - 1),\delta + OPT(i - 1, j), \delta + OPT(i, j - 1)]$ \\ 
$(i,j)$ is part of the optimal solution only if the first term is the minimum.

\columnbreak

Algorithm:
\begin{itemize}
    \item Array [m, n]
    \item initialize $A[i, 0] = i\delta$
    \item initialize $A[0, j] = j\delta$
    \item for j=1...n
    \item \quad for i=1...m
    \item \quad \quad Use recurrence
    \item Return A[m, n]
\end{itemize}
\end{multicols}

\noindent The output of $OPT$ is typically memoized into a 2d array, and results in a runtime of $O(m * n)$ where $m$ and $n$ represent
the length of each of the strings.

\subsection{Bellman-Ford Algorithm}
\begin{multicols}{2}
Accomplishes the same thing as Djikstra's algorithms while also allowing for negative
weight edges. The recurrence for solving this is on the right.
Where $w$ represents an intermediate vertex, $c_{vw}$ represents the cost from going from $v$ to $w$,
$v$ represents the starting vertex, and $i$ represents the length of the path.
\columnbreak

$OPT(n) = \begin{cases}
    OPT(i-1, v)  & P \text{ uses at most } i-1 \text{edges} \\
    OPT(i-1, w) + c_{vw} & P \text{ uses at most } i \text{ edges}
  \end{cases}$
\end{multicols}

\section{Network Flow}
Graphs are sometimes used to model transportation networks. In
such networks, edges carry some sort of traffic and nodes act like switches passing the
traffic between edges. For example, in a computer network, the edges are links that carry
the data and the nodes are switches. Network flow are used to model such transportation
networks.

\subsection{Maximum Flow Problem}
The max flow problem represents a problem in which we want to find the maximum flow between
2 edges (a source, typically denoted with $s$, and a sink, typically denoted with $t$). These
are generally proved by verifying the capacity and conservation conditions.

\subsection{Ford-Fulkerson Algorithm}
The Ford-Fulkerson Algorithm works by finding a simple path $p$ beetween $s$ and $t$, then finding
the edge with the least capacity on that path and augmenting each node with that value.
Note that you can go against the direction of an edge in the graph so long as you subtract when augmenting
it's capacity. This is then repeated until no paths between $s$ and $t$ with a nonzero capacity exist.
This algorithm has a time complexity of O(mC), where m is the number of edges and C is the max flow
value.


To prove maximum flow problems, you have to verify the capacity and conservation conditions. 
To prove capacity, you prove that $0 \leq f'(e) \leq c_e$ where f' is a new flow and $c_e$ is the 
capacity of the edge. There are two cases, the forward and backward edge. You prove the $c_e$ 
condition for forward edges and the 0
condition for backward edges. For the conservation condition, you need to check the change in 
the amount of flow entering v is the same as the change in the amount of flow exiting v. There 
There are four cases to check depending on whether the edge that enters v is a forward or a backward 
edge, and whether the edge that exits v is a forward or a backward 
edge. Note that a forward edge entering v means that you add b to the flow value while 
backward edge at v means you subtract b from the flow value.

\subsection{Maximum Flow and Minimum Cut in a Network}
The min cut refers to the minimum set of edges that would have to be removed such that there is no longer
a path from $s$ to $t$. The capacity of the cut edges is equal to the maximum flow of the network.
You create a min-cut by drawing a line from top to bottom across the network. There are two distinct
notations used to describe min-cuts. The A-B notation says that if you have a set of vertices in A
and one in B, the min cut == max flow is the sum of all the edges intersected by your drawn line to
create those groups. You can also specify it based on the edges themselves, creating a set of edges
that will be intersected. 

\section{Misc. Information}
Runtimes:
- Pseudo-polynomial time: When the runtime is based of the size of the input, for example checking
if a number is prime. Taking the size of the input in bits into consideration, the algorithm is
exponential, but ignoring that it seems as if it is linear. 


\subsection{Induction w/ two variables}
\begin{itemize}
    \item Prove induction on i+j
    \item Base Case: When i+j = 0
    \item Inductive Hypothesis: Assume that for $0 \le i' + j' < i+j$, $OPT(i', j')$ is correct
    \item Inductive step: Prove  $OPT(i, j)$
\end{itemize}




\end{document}
